# **🤖 트랜스포머(Transformer) 인코더 & 디코더 핵심 요약**

이 문서는 트랜스포머 모델의 두 축인 \*\*인코더(Encoder)\*\*와 \*\*디코더(Decoder)\*\*의 역할과 구조를 초등학생도 이해할 수 있는 비유를 섞어 설명합니다.

## **1\. 인코더 (Encoder): "이해하는 팀" 🧠**

인코더의 목적은 입력된 문장의 맥락을 깊이 있게 파악하여 핵심 정보를 추출하는 것입니다.

### **🏗️ 구조 (N=6개의 층)**

각 층은 두 개의 하위 층으로 구성됩니다.

1. **멀티헤드 셀프 어텐션 (Self-Attention):** 문장 속 단어들 사이의 관계를 파악합니다.  
   * *비유:* 여러 명의 전문가가 문장을 읽고, 각 단어가 서로에게 얼마나 중요한지 각자의 관점에서 분석하는 것.  
2. **피드포워드 네트워크 (FFN):** 추출된 정보를 개별적으로 더 세련되게 가공합니다.  
   * *비유:* 분석된 정보들을 작업실에서 더 멋지게 다듬는 공장.

### **🛠️ 보조 장치**

* **잔차 연결 (Residual Connection):** 원본 정보를 복사해서 전달하여 정보 손실을 방지합니다.  
* **층 정규화 (Layer Normalization):** 정보의 크기를 일정하게 맞춰 학습을 안정화합니다.  
* **차원 (**$d\_{model} \= 512$**):** 모든 정보는 512개의 숫자로 이루어진 '정보 덩어리'로 표현됩니다.

## **2\. 디코더 (Decoder): "생성하는 팀" ✍️**

디코더의 목적은 인코더가 정리한 정보를 바탕으로 새로운 문장을 한 단어씩 만들어내는 것입니다.

### **🏗️ 구조 (N=6개의 층)**

인코더와 비슷하지만, 한 단계가 더 추가되어 세 개의 하위 층을 가집니다.

1. **마스크드 셀프 어텐션 (Masked Self-Attention):** 단어를 예측할 때 아직 나오지 않은 '미래의 단어'를 보지 못하게 가립니다.  
   * *비유:* 글을 쓸 때 뒷부분 내용을 미리 보지 못하게 눈을 가리고 순서대로 쓰는 것.  
2. **인코더-디코더 어텐션:** 인코더가 요약한 정보와 자신이 지금까지 쓴 내용을 연결합니다.  
   * *비유:* 원본 자료(인코더)를 참고하면서 내가 쓴 글(디코더)의 다음 내용을 결정하는 것.  
3. **피드포워드 네트워크 (FFN):** 생성된 정보를 최종적으로 가공합니다.

## **3\. 번역 과정 예시 (I eat an apple → 나는 사과를 먹는다)**

1. **인코더 단계:**  
   * "I eat an apple" 문장을 분석합니다.  
   * 'I'는 주어, 'eat'은 동사, 'apple'은 목적어라는 관계를 파악해 512차원의 정보 덩어리로 만듭니다.  
2. **디코더 단계:**  
   * 시작 토큰을 받고 인코더의 정보를 참고해 \*\*"나는"\*\*을 예측합니다.  
   * "나는"과 인코더 정보를 합쳐 \*\*"사과를"\*\*을 예측합니다.  
   * "나는 사과를"과 인코더 정보를 합쳐 \*\*"먹는다"\*\*를 예측하며 문장을 완성합니다.

## **💡 요약 포인트**

| 구분 | 인코더 (Encoder) | 디코더 (Decoder) |
| :---- | :---- | :---- |
| **핵심 역할** | 입력 문장의 의미 이해 및 추출 | 이해한 정보를 바탕으로 새로운 문장 생성 |
| **주요 특징** | 모든 단어를 동시에 보고 관계 파악 | 미래의 단어를 가리고(Masking) 순차적으로 생성 |
| **하위 층 수** | 2개 | 3개 (인코더 정보를 받는 층 추가) |

이 구조를 통해 트랜스포머는 문맥을 정확하게 파악하고 자연스러운 문장을 만들어낼 수 있습니다.