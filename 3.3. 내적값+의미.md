# **트랜스포머 어텐션(Attention) 메커니즘: 내적의 의미와 역할**

트랜스포머 모델의 핵심인 '어텐션'은 수많은 정보 중 \*\*"지금 어떤 정보에 집중할 것인가?"\*\*를 결정하는 장치입니다. 질문하신 내적($QK^T$)의 결과는 단어 사이의 '관계의 깊이'를 수치로 나타낸 것입니다.

## **1\. Q, K, V의 역할: 도서관 비유**

이해를 돕기 위해 어텐션을 **도서관에서 책을 찾는 과정**에 비유해 보겠습니다.

* **Query (**$Q$**): 찾고자 하는 주제 (질문)**  
  * 예: "사과 요리법에 대해 알고 싶어."  
* **Key (**$K$**): 책장에 붙은 카테고리 태그 (색인)**  
  * 예: \[과일 요리\], \[전자제품\], \[운동법\], \[디저트\]  
* **Value (**$V$**): 책 안에 들어있는 실제 정보 (내용)**  
  * 예: "사과 파이 만드는 법은...", "아이폰 수리법은..."

**내적(**$QK^T$**)의 과정**은 내가 들고 있는 질문($Q$)과 도서관의 태그($K$)들을 하나씩 대조해보며 \*\*"이 책이 내가 찾는 내용과 얼마나 관련 있는가?"\*\*를 점수로 매기는 과정입니다.

## **2\. 구체적인 문장 예시: "철수가 사과를 맛있게 먹었다"**

모델이 이 문장을 처리할 때, '먹었다'라는 단어가 문맥을 어떻게 파악하는지 살펴보겠습니다.

### **단계 1: 연관성 계산 (Dot Product)**

'먹었다'가 \*\*Query($Q$)\*\*가 되어 주변 단어들의 \*\*Key($K$)\*\*와 내적을 수행합니다.

* **'먹었다'(**$Q$**) · '철수가'(**$K$**)**: "누가 먹었지?" $\\rightarrow$ 연관성 있음 (높은 점수)  
* **'먹었다'(**$Q$**) · '사과를'(**$K$**)**: "무엇을 먹었지?" $\\rightarrow$ **매우 강한 연관성 (가장 높은 점수)**  
* **'먹었다'(**$Q$**) · '맛있게'(**$K$**)**: "어떻게 먹었지?" $\\rightarrow$ 연관성 있음 (중간 점수)

### **단계 2: 가중치 배분 (Scaling & Softmax)**

계산된 점수들을 확률(0\~1 사이)로 변환합니다.

* 철수가: 0.2  
* **사과를: 0.7**  
* 맛있게: 0.1  
  (합계: 1.0)

### **단계 3: 최종 정보 조합 (Weighted Sum)**

이제 각 단어가 가진 의미 정보인 \*\*Value($V$)\*\*에 위에서 구한 확률을 곱해서 다 더합니다.

* 최종 결과물(Attention Value)은 '사과'의 의미 정보를 가장 많이 포함하게 되며, 이를 통해 모델은 \*\*"'먹었다'는 행위는 '사과'라는 대상과 결합된 동작이다"\*\*라는 문맥적 결론에 도달합니다.

## **3\. 왜 하필 '내적(Dot Product)'인가?**

### **기하학적 의미: 방향의 일치**

두 벡터의 내적 공식은 $A \\cdot B \= |A||B|\\cos\\theta$입니다.

* $\\cos\\theta$**가 1에 가까움 (0도):** 두 벡터가 같은 방향을 향함 $\\rightarrow$ **유사함**  
* $\\cos\\theta$**가 0에 가까움 (90도):** 두 벡터가 직교함 $\\rightarrow$ **상관없음**

트랜스포머는 학습을 통해 **함께 자주 쓰이거나 문법적으로 연결된 단어들을 고차원 공간에서 비슷한 방향을 바라보도록** 위치시킵니다. 따라서 내적 값이 크다는 것은 두 단어가 같은 맥락(Context) 안에 있다는 수학적 증거가 됩니다.

### **수치적 지표로서의 역할**

내적은 두 벡터가 얼마나 닮았는지를 하나의 숫자로 요약해 줍니다. 이 숫자가 크면 "어텐션을 많이 줘라(집중해라)", 작으면 "무시해라"라는 명령이 되어 모델이 효율적으로 정보를 처리할 수 있게 합니다.

## **요약**

1. **내적(**$QK^T$**):** 질문($Q$)과 정보의 태그($K$) 사이의 유사도를 측정하는 **검색 엔진**의 역할입니다.  
2. **Softmax:** 검색 결과 중 어떤 정보에 몇 %의 비중을 둘지 결정하는 **의사결정** 과정입니다.  
3. **결과:** "이 단어를 이해하려면 저 단어를 70% 참고해\!"라는 **수치적 가이드라인**을 제공하여 문맥이 풍부한 벡터 표현을 만들어냅니다.