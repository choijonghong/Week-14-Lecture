# **트랜스포머(Transformer) 아키텍처 개요**

트랜스포머는 2017년 Google Brain 팀의 논문 \*\*"Attention Is All You Need"\*\*에서 제안된 모델로, RNN이나 CNN 없이 오직 **어텐션(Attention)** 메커니즘만을 사용하여 시퀀스 데이터를 처리하는 혁신적인 구조입니다.

## **1\. 전체 구조: 인코더-디코더 (Encoder-Decoder)**

트랜스포머는 기본적으로 입력 정보를 처리하는 인코더와 결과를 생성하는 디코더로 나뉩니다.

* **인코더(Encoder):** 입력 시퀀스를 문맥 정보를 담은 벡터 표현으로 변환합니다.  
* **디코더(Decoder):** 인코더의 정보와 이전에 생성된 단어들을 참조하여 다음 단어를 예측합니다.

## **2\. 인코더(Encoder)의 구성**

인코더는 $N$개의 동일한 계층이 쌓인 구조이며, 각 계층은 두 개의 서브 계층으로 이루어집니다.

### **2.1 멀티-헤드 셀프-어텐션 (Multi-Head Self-Attention)**

문장 내 단어 간의 관계를 파악합니다. 세 가지 벡터(Query, Key, Value)를 사용하여 연산합니다.

* **Attention Score:** $Q$와 $K$의 유사도를 계산하여 소프트맥스로 정규화합니다.  
* **수식:**$$Attention(Q, K, V) \= \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d\_k}}\\right)V$$  
* **Multi-Head:** 여러 개의 어텐션을 병렬로 수행하여 다양한 문맥적 관계를 동시에 학습합니다.

### **2.2 포지션-와이즈 피드-포워드 네트워크 (Position-wise FFN)**

각 위치에서 독립적으로 적용되는 완전 연결 신경망입니다.

* 수식:  
  $$FFN(x) \= \\max(0, xW\_1 \+ b\_1)W\_2 \+ b\_2$$

## **3\. 디코더(Decoder)의 구성**

디코더는 인코더의 요소에 하나가 더 추가된 세 개의 서브 계층으로 구성됩니다.

1. **마스크드 멀티-헤드 셀프-어텐션 (Masked Multi-Head Self-Attention):** 미래의 단어를 미리 보지 못하도록 마스킹 처리하여 현재까지 생성된 단어들만 참조합니다.  
2. **인코더-디코더 어텐션 (Encoder-Decoder Attention):** 디코더가 인코더의 출력 정보 중 어떤 부분에 집중할지 결정합니다. ($Q$는 디코더에서, $K$와 $V$는 인코더에서 가져옴)  
3. **포지션-와이즈 피드-포워드 네트워크:** 인코더와 동일한 역할을 수행합니다.

## **4\. 포지셔널 인코딩 (Positional Encoding)**

트랜스포머는 순차적으로 데이터를 처리하지 않으므로 단어의 위치 정보를 별도로 알려줘야 합니다. 이를 위해 사인(Sine)과 코사인(Cosine) 함수를 이용한 벡터를 임베딩에 더해줍니다.

* 수식:  
  $$PE\_{(pos, 2i)} \= \\sin(pos / 10000^{2i/d\_{model}})$$$$PE\_{(pos, 2i+1)} \= \\cos(pos / 10000^{2i/d\_{model}})$$

## **5\. 핵심 특징 요약**

| 특징 | 설명 |
| :---- | :---- |
| **병렬 처리** | RNN과 달리 전체 시퀀스를 한 번에 처리하여 학습 속도가 매우 빠름 |
| **장기 의존성** | 어텐션을 통해 멀리 떨어진 단어 간의 관계도 효과적으로 포착 |
| **잔차 연결** | 각 서브 계층 출력에 $LayerNorm(x \+ Sublayer(x))$를 적용하여 정보 손실 방지 |

트랜스포머는 현재 GPT, BERT 등 현대 생성형 AI 모델의 근간이 되는 가장 중요한 아키텍처입니다.