# **어텐션(Attention) 메커니즘**

어텐션 메커니즘은 모델이 입력된 정보 중 \*\*가장 중요한 부분에 '집중'\*\*하도록 돕는 기술입니다. 문장을 이해할 때 모든 단어를 동일하게 취급하지 않고, 맥락에 따라 중요한 단어에 더 큰 비중을 두는 인간의 인지 능력을 모방한 것입니다.

## **1\. 쿼리(Q), 키(K), 밸류(V)의 관계**

어텐션을 이해하는 가장 쉬운 방법은 **도서관에서 책을 찾는 과정**에 비유하는 것입니다.

* **쿼리(Query, Q):** "내가 찾고 싶은 정보가 무엇인가?" (예: 질문)  
* **키(Key, K):** "정보의 제목이나 색인이 무엇인가?" (예: 책의 제목, 목차)  
* **밸류(Value, V):** "정보의 실제 내용은 무엇인가?" (예: 책의 본문)

**핵심 원리:** 질문(Q)과 가장 잘 맞는 제목(K)을 찾아, 그와 연결된 실제 내용(V)을 가져오는 과정입니다.

## **2\. 스케일드 닷-프로덕트 어텐션 (Scaled Dot-Product Attention)**

가장 기본적인 계산 방식으로, 다음 4단계를 거칩니다.

1. **유사도 계산:** 쿼리(Q)와 키(K)를 곱해(Dot Product) 얼마나 비슷한지 점수를 매깁니다.  
2. **스케일링(Scaling):** 점수가 너무 커지지 않게 차원의 크기($\\sqrt{d\_k}$)로 나눠 조절합니다.  
3. **소프트맥스(Softmax):** 점수를 확률(합계가 1)로 변환하여 각 정보의 '가중치'를 구합니다.  
4. **최종 결과:** 가중치를 실제 내용(V)에 곱해 모두 더합니다. 중요한 정보는 많이, 불필요한 정보는 적게 반영된 결과물이 나옵니다.

## **3\. 멀티-헤드 어텐션 (Multi-Head Attention)**

한 번에 여러 개의 어텐션을 동시에 사용하는 방식입니다.

* 여러 명의 전문가(Head)가 각자의 관점(문법적 관계, 의미적 관계 등)에서 분석한 뒤, 그 결과를 하나로 합쳐 정보를 더 풍부하고 정확하게 이해합니다.

## **4\. 트랜스포머 모델에서의 활용**

트랜스포머는 어텐션을 세 가지 방식으로 영리하게 사용합니다.

| 종류 | 설명 | 예시 |
| :---- | :---- | :---- |
| **인코더-디코더 어텐션** | 출력 단어를 만들 때 입력 문장의 어떤 단어를 참고할지 결정 | 번역 시 'student'를 만들 때 '학생'에 집중 |
| **인코더 셀프-어텐션** | 입력 문장 내부 단어들끼리의 관계를 파악 | "강가의 은행"에서 '은행'이 '둑'임을 파악 |
| **디코더 셀프-어텐션** | 이미 생성된 이전 단어들만 참고 (미래 단어 마스킹) | 다음에 올 정답을 미리 보지 못하게 차단 |

## **5\. 결론**

어텐션은 \*\*"내 질문에 맞는 특징을 찾아서, 거기 연결된 실제 정보를 가져오는 과정"\*\*입니다. 이를 통해 AI는 문맥을 훨씬 더 잘 파악하고 자연스러운 결과를 만들어낼 수 있게 되었습니다.